{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b389a05-a11e-432c-a731-b956ff5536b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Insurance Agent with UC Tools\n",
    "\n",
    "* Framework: [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb46e8f9-088b-44ef-8bbf-eb00acaf2913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt -q\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:14.416506Z",
     "start_time": "2025-05-12T06:31:14.315131Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c64825c-fa8f-4f64-b53c-faaf82fb279a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:19.413138Z",
     "start_time": "2025-05-12T06:31:15.206931Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "087fc690-09a7-4e9d-bf69-968f2ead5fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "#import from shared helper which s 2 level above\n",
    "import importlib\n",
    "shared = importlib.import_module(\"shared\")\n",
    "from shared.helper import *\n",
    "\n",
    "in_workspace = is_running_in_databricks()\n",
    "global_config = get_global_config('../../global_config/databricks_config.yaml')\n",
    "\n",
    "if not in_workspace:\n",
    "    print(\"running locally ...\")\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    load_dotenv('../../.env')\n",
    "\n",
    "    DATABRICKS_HOST = os.getenv('host')\n",
    "    DATABRICKS_TOKEN = os.getenv('token')\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    # local mlflow setup\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_registry_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"open_agent_sdk_experiments\")\n",
    "else:\n",
    "    print(\"running in workspace ...\")\n",
    "    DATABRICKS_HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"databricks_token_qyu\", key=\"OpenAi\")\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    mlflow.set_experiment(\"/Users/q.yu@databricks.com/ML_experiments/insurance_operator_openai_agents_sdk\")\n",
    "\n",
    "print(f\"host: {DATABRICKS_HOST}\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "mlflow.openai.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5d8fe8-9ee4-4880-a8fa-821276d105b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:22.903286Z",
     "start_time": "2025-05-12T06:31:22.750208Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819651f0-6b68-49b8-bf55-80eee5b4b07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    cust_id: str | None = None\n",
    "    policy_no: str | None = None\n",
    "    conversation_id: str | None = None\n",
    "    user_id: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:28.508517Z",
     "start_time": "2025-05-12T06:31:28.463437Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f74e320-9867-489a-ad54-9d6a1710350f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unitycatalog.ai.core.databricks import (\n",
    "    DatabricksFunctionClient,\n",
    "    FunctionExecutionResult,\n",
    ")\n",
    "from agents import function_tool, RunContextWrapper\n",
    "\n",
    "catalog = global_config.get(\"catalog\")\n",
    "schema = global_config.get(\"schema_insurance_agents\")\n",
    "\n",
    "@function_tool\n",
    "def search_claims_details_by_policy_no(wrapper: RunContextWrapper[UserInfo]) -> FunctionExecutionResult:\n",
    "    print(\"[DEBUG]: the 'search_claims_details_by_policy_no' tool was called\")\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=f\"{catalog}.{schema}.search_claims_details_by_policy_no\",\n",
    "        parameters={\"input_policy_no\": wrapper.context.policy_no},\n",
    "    )\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def policy_docs_vector_search(query: str) -> FunctionExecutionResult:\n",
    "    print(\"[DEBUG]: the 'policy_docs_vector_search' tool was called\")\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=f\"{catalog}.{schema}.policy_docs_vector_search\",\n",
    "        parameters={\"query\": query},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c74df770-6ee0-47ea-8d18-418e813359de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:31.786878Z",
     "start_time": "2025-05-12T06:31:31.749284Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a36beeb-989c-4e8f-8c2f-60fcd4efc64b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you want custom model hosted outside of OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "from agents import OpenAIChatCompletionsModel, set_tracing_disabled\n",
    "\n",
    "# You can replace 'gtp-4o' with the MODEL variable in the Agent definition\n",
    "MODEL = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=f\"https://{DATABRICKS_HOST}/serving-endpoints\",\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52001b04-cde2-45ac-a9f3-48fea835d8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The instruction field is the prompt for the agent. We’ve added a `RECOMMENDED_PROMPT_PREFIX` object from OpenAI. This is a optional “hand-off” prompt, and we found it to work fairly well. The tools field is where you designate a list of tools that the agent can use to complete the task specified in the instruction. The model field is where you specify the LLM that powers the agent. It works natively with all OpenAI models, provided that you have access to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:33:30.013195Z",
     "start_time": "2025-05-12T06:33:29.852262Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af856cc-cce3-44dc-ad7e-808809971a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "\n",
    "RECOMMENDED_PROMPT_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:33:32.693336Z",
     "start_time": "2025-05-12T06:33:32.666964Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da7c66b-a35e-4cbc-b02a-024ce2bd6f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "\n",
    "# You can turn off trace by setting this to True\n",
    "set_tracing_disabled(disabled=False)\n",
    "\n",
    "claims_detail_retrieval_agent = Agent[UserInfo](\n",
    "    name=\"Claims Details Retrieval Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a claims details retrieval agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer. \\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to retrieve data about a claim. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[\n",
    "        search_claims_details_by_policy_no,\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    # model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),\n",
    ")\n",
    "\n",
    "policy_qa_agent = Agent[UserInfo](\n",
    "    name=\"Policy Q&A Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are an insurance policy Q&A agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer.\\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to answer the question about their policy. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[policy_docs_vector_search],\n",
    "    model=\"gpt-4o\",\n",
    "    # model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),\n",
    ")\n",
    "\n",
    "triage_agent = Agent[UserInfo](\n",
    "    name=\"Triage agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a helpful triaging agent. \"\n",
    "        \"You can use your tools to delegate questions to other appropriate agents. \"\n",
    "        \"If the customer does not have anymore questions, wish them a goodbye and a good rest of their day. \"\n",
    "    ),\n",
    "    # handoffs=[customer_verifier_agent, claims_detail_retrieval_agent, policy_qa_agent],\n",
    "    handoffs=[claims_detail_retrieval_agent, policy_qa_agent],\n",
    "    model=\"gpt-4o\",\n",
    "    # model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "392e61ea-9954-4cda-b051-7e968bf1129c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment a Chat loop with Mlflow\n",
    "\n",
    "Sample Conversation:\n",
    "* \"hi, id like to check on my existing claims\"\n",
    "* \"here's the policy number: 102070455\"\n",
    "* \"sure. id like to ask a different question. does my policy cover towing and labor costs?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:34:33.484988Z",
     "start_time": "2025-05-12T02:34:33.293745Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4572519-b109-4ee6-9963-7dc946197b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Input some user data as context\n",
    "user_info = UserInfo(cust_id=\"7852\", policy_no=\"102070455\", coversation_id=\"123\", user_id=\"123\")\n",
    "user_input = \"[USER]: I'like to check on my existing claims\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T03:11:59.863545Z",
     "start_time": "2025-05-12T03:10:39.134011Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9345cb-7f77-4b44-9f32-55701dffa315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start a chat span\n",
    "with mlflow.start_span(name=\"insurance_agent\", span_type=\"AGENT\") as span:\n",
    "    print(\"[AGENT] Hello! How may I assist you?\")\n",
    "    result = None\n",
    "    while True:\n",
    "        user_input = input(\"[USER]: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"[AGENT]: Bye!\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            continue\n",
    "        try:\n",
    "            if result:\n",
    "                user_input = result.to_input_list() + [{\"role\": \"user\", \"content\": user_input}]\n",
    "            result = await Runner.run(\n",
    "                starting_agent=triage_agent, input=user_input, context=user_info\n",
    "            )\n",
    "            print(\"\\n[AGENT]:\", result.final_output)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6955e74-dc41-4c83-8db7-db71f42359f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write the agent to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:33:40.865014Z",
     "start_time": "2025-05-12T06:33:40.837875Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8010a12f-b6f6-48fd-bad2-0607f83ae257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile insurance_chat_agent.py\n",
    "from typing import Any, List, Optional, Dict, Generator\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "import mlflow\n",
    "from uuid import uuid4\n",
    "import asyncio\n",
    "from pydantic import BaseModel\n",
    "from unitycatalog.ai.core.databricks import (\n",
    "    DatabricksFunctionClient,\n",
    "    FunctionExecutionResult,\n",
    ")\n",
    "from agents import OpenAIChatCompletionsModel, set_tracing_disabled\n",
    "from agents import function_tool, RunContextWrapper\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"insurance_chat_agent\")\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    cust_id: str | None = None\n",
    "    policy_no: str | None = None\n",
    "    conversation_id: str | None = None\n",
    "    user_id: str | None = None\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_claims_details_by_policy_no(wrapper: RunContextWrapper[UserInfo], policy_no: str) -> FunctionExecutionResult:\n",
    "    logger.info(\"The 'search_claims_details_by_policy_no' tool was called\")\n",
    "    wrapper.context.policy_no = policy_no\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=\"ai.insurance_agent.search_claims_details_by_policy_no\",\n",
    "        parameters={\"input_policy_no\": wrapper.context.policy_no},\n",
    "    )\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def policy_docs_vector_search(query: str) -> FunctionExecutionResult:\n",
    "    logger.info(\"The 'policy_docs_vector_search' tool was called\")\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=\"ai.insurance_agent.policy_docs_vector_search\",\n",
    "        parameters={\"query\": query},\n",
    "    )\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "\n",
    "claims_detail_retrieval_agent = Agent[UserInfo](\n",
    "    name=\"Claims Details Retrieval Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a claims details retrieval agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer. \\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to retrieve data about a claim. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[\n",
    "        search_claims_details_by_policy_no,\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    #model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client)\n",
    ")\n",
    "\n",
    "policy_qa_agent = Agent[UserInfo](\n",
    "    name=\"Policy Q&A Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are an insurance policy Q&A agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer.\\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to answer the question about their policy. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[policy_docs_vector_search],\n",
    "    model=\"gpt-4o\",\n",
    "    #model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client)\n",
    ")\n",
    "\n",
    "triage_agent = Agent[UserInfo](\n",
    "    name=\"Triage agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a helpful triaging agent. \"\n",
    "        \"You can use your tools to delegate questions to other appropriate agents. \"\n",
    "        \"If the customer does not have anymore questions, wish them a goodbye and a good rest of their day. \"\n",
    "    ),\n",
    "    handoffs=[claims_detail_retrieval_agent, policy_qa_agent],\n",
    "    model=\"gpt-4o\",\n",
    "    #model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client)\n",
    ")\n",
    "\n",
    "class InsuranceChatAgent(ChatAgent):\n",
    "    def __init__(self, starting_agent: Agent):\n",
    "        self.starting_agent = starting_agent\n",
    "        self.conversation_state = {}\n",
    "\n",
    "    def _get_or_create_conversation_state(self, conversation_id: str):\n",
    "        \"\"\"Get or create the state for a conversation\"\"\"\n",
    "        if conversation_id not in self.conversation_state:\n",
    "            self.conversation_state[conversation_id] = {\n",
    "                \"current_agent\": self.starting_agent,\n",
    "                \"conversation_history\": None\n",
    "            }\n",
    "        return self.conversation_state[conversation_id]\n",
    "\n",
    "    def _get_latest_user_message(selfself, messages: List[ChatAgentMessage]) -> str:\n",
    "        \"\"\"Extract the most recent user messages as input text\"\"\"\n",
    "        for message in reversed(messages):\n",
    "            if message.role == \"user\":\n",
    "                return message.content\n",
    "            return \"\"\n",
    "\n",
    "    def _create_user_context(\n",
    "            self,\n",
    "            context: Optional[ChatContext] = None,\n",
    "            custom_inputs: Optional[Dict[str, Any]] = None\n",
    "        ) -> UserInfo:\n",
    "        \"\"\"Convert MLflow inputs to UserInfo object\"\"\"\n",
    "        user_info = UserInfo()\n",
    "\n",
    "        if context:\n",
    "            conversation_id = getattr(context, \"conversation_id\", None)\n",
    "            if conversation_id:\n",
    "                user_info.conversation_id = conversation_id\n",
    "\n",
    "            user_id = getattr(context, \"user_id\", None)\n",
    "            if user_id:\n",
    "                user_info.user_id = user_id\n",
    "\n",
    "        return user_info\n",
    "\n",
    "    @mlflow.trace(name=\"insurance_chat_agent\", span_type=SpanType.AGENT)\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[Dict[str, Any]] = None\n",
    "    ) -> ChatAgentResponse:\n",
    "        user_info = self._create_user_context(context, custom_inputs)\n",
    "        conversation_id = user_info.conversation_id\n",
    "\n",
    "        # Get the state for this conversation\n",
    "        state = self._get_or_create_conversation_state(conversation_id)\n",
    "        current_agent = state[\"current_agent\"]\n",
    "        conversation_history = state[\"conversation_history\"]\n",
    "\n",
    "\n",
    "        # Get the latest user message\n",
    "        latest_message = self._get_latest_user_message(messages)\n",
    "\n",
    "        # Prepare the input for the agent\n",
    "        if conversation_history is None:\n",
    "            # First turn, just use the latest message\n",
    "            agent_input = latest_message\n",
    "        else:\n",
    "            # Add the new user message to the conversation history\n",
    "            conversation_history.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": latest_message\n",
    "            })\n",
    "            agent_input = conversation_history\n",
    "\n",
    "        # Run the agent use asyncio\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        try:\n",
    "            result = loop.run_until_complete(\n",
    "                Runner.run(\n",
    "                    starting_agent=self.starting_agent,\n",
    "                    input=agent_input,\n",
    "                    context=user_info,\n",
    "                )\n",
    "            )\n",
    "            # Update the state for the next turn\n",
    "            # Store the updated conversation history from the result\n",
    "            state[\"conversation_history\"] = result.to_input_list()\n",
    "\n",
    "            # Update the current agent based on which agent was last used\n",
    "            if hasattr(result, \"last_agent\") and result.last_agent:\n",
    "                state[\"current_agent\"] = result.last_agent\n",
    "\n",
    "        finally:\n",
    "            loop.close()\n",
    "\n",
    "        # Convert the result to ChatAgentResponse format:\n",
    "        return ChatAgentResponse(\n",
    "            messages=[\n",
    "                ChatAgentMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=result.final_output,\n",
    "                    id=str(uuid4())\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @mlflow.trace(name=\"insurance_change_agent_stream\", span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[Dict[str, Any]] = None\n",
    "    ) -> Generator[ChatAgentResponse, None, None]:\n",
    "        response = self.predict(messages, context, custom_inputs)\n",
    "\n",
    "        # Yield it as a single chunk\n",
    "        for message in response.messages:\n",
    "            yield ChatAgentChunk(delta=message)\n",
    "\n",
    "AGENT = InsuranceChatAgent(starting_agent=triage_agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e407fa09-dba2-4d91-9f80-8ee0e4d2c8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Agent, Run Validation, log and Register the Agent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a41e6c-fb38-40ac-8008-ef97e2b30c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:37:42.561469Z",
     "start_time": "2025-05-12T06:37:30.167421Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239747e3-414c-40d8-b093-5b8fb414ed8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from insurance_chat_agent import AGENT\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"databricks_token_qyu\", key=\"OpenAi\")\n",
    "\n",
    "AGENT.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"hi, id like to check on my existing claims and my policy number: 102070455\",\n",
    "            }\n",
    "        ],\n",
    "        \"context\": {\"conversation_id\": \"123\", \"user_id\": \"123\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efa0dfa-3d96-4b49-9570-98578c20cefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AGENT.predict({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"does my policy cover towing and labor costs?\"}],\n",
    "        \"context\": {\"conversation_id\": \"123\", \"user_id\": \"123\"}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd82833a-470a-4991-bdf9-55a1ac530150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29464776-566b-491a-8bb2-90a1477527e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksFunction,\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksVectorSearchIndex)\n",
    "from unitycatalog.ai.openai.toolkit import UCFunctionToolkit\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\n",
    "    scope=\"my_secret_scope\", key=\"OpenAi\"\n",
    ")\n",
    "\n",
    "resources = [\n",
    "    DatabricksVectorSearchIndex(\n",
    "        index_name=\"ai.agents.policy_docs_chunked_files_vs_index\"\n",
    "    ),\n",
    "    DatabricksServingEndpoint(endpoint_name=\"databricks-bge-large-en\"),\n",
    "    DatabricksFunction(\n",
    "        function_name=\"ai.insurance_agent.search_claims_details_by_policy_no\"\n",
    "    ),\n",
    "    DatabricksFunction(\n",
    "        function_name=\"ai.insurance_agent.policy_docs_vector_search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{my_databricks_account}/ML_experiments/insurance_chat_agent\")\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Log the model using the \"models from code\" approach\n",
    "with mlflow.start_run():\n",
    "    logged_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"insurance_chat_agent\",\n",
    "        python_model=os.path.join(os.getcwd(), \"insurance_chat_agent.py\"),\n",
    "        input_example={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"hi, id like to check on my existing claims?\",\n",
    "                }\n",
    "            ],\n",
    "            \"context\": {\"conversation_id\": \"123\", \"user_id\": \"123\"},\n",
    "        },\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"openai-agents\",\n",
    "            \"unitycatalog-openai[databricks]==0.2.0\",\n",
    "            \"pydantic\",\n",
    "        ],\n",
    "        resources=resources\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1412556571878803,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04-1-openai-agents-sdk-agents",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b389a05-a11e-432c-a731-b956ff5536b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Insurance Agent with UC Tools\n",
    "\n",
    "* Framework: [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:14.416506Z",
     "start_time": "2025-05-12T06:31:14.315131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:19.413138Z",
     "start_time": "2025-05-12T06:31:15.206931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "#import from shared helper which s 2 level above\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir)))\n",
    "from shared.helper import *\n",
    "\n",
    "in_workspace = is_running_in_databricks()\n",
    "global_config = get_global_config('../../global_config/databricks_config.yaml')\n",
    "\n",
    "if not in_workspace:\n",
    "    print(\"running locally ...\")\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    load_dotenv('../../.env')\n",
    "\n",
    "    DATABRICKS_HOST = os.getenv('host')\n",
    "    DATABRICKS_TOKEN = os.getenv('token')\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    # local mlflow setup\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_registry_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"open_agent_sdk_experiments\")\n",
    "else:\n",
    "    print(\"running in workspace ...\")\n",
    "    DATABRICKS_HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"databricks_token_qyu\", key=\"OpenAi\")\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    mlflow.set_experiment(\"/Users/q.yu@databricks.com/ML_experiments/insurance_operator_openai_agent_v2\")\n",
    "\n",
    "print(f\"host: {DATABRICKS_HOST}\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "mlflow.openai.autolog()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helper:{'catalog': 'fins_genai', 'schema_agents': 'agents', 'schema_insurance_agents': 'insurance_agent', 'llm_endpoint_llama3p3_70B': 'databricks/databricks-meta-llama-3-3-70b-instruct', 'llm_endpoint_llama4_maverick': 'databricks/databricks-llama-4-maverick', 'llm_endpoint_sonnet_3p7': 'databricks/databricks-claude-3-7-sonnet'}\n",
      "INFO:databricks.sdk:loading DEFAULT profile from ~/.databrickscfg: host, serverless_compute_id, connection_timeout_seconds, token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running locally ...\n",
      "host: adb-984752964297111.11.azuredatabricks.net\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5d8fe8-9ee4-4880-a8fa-821276d105b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Define Tools"
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819651f0-6b68-49b8-bf55-80eee5b4b07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:22.903286Z",
     "start_time": "2025-05-12T06:31:22.750208Z"
    }
   },
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    cust_id: str | None = None\n",
    "    policy_no: str | None = None\n",
    "    conversation_id: str | None = None\n",
    "    user_id: str | None = None"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f74e320-9867-489a-ad54-9d6a1710350f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:28.508517Z",
     "start_time": "2025-05-12T06:31:28.463437Z"
    }
   },
   "source": [
    "from unitycatalog.ai.core.databricks import (\n",
    "    DatabricksFunctionClient,\n",
    "    FunctionExecutionResult,\n",
    ")\n",
    "from agents import function_tool, RunContextWrapper\n",
    "\n",
    "catalog = global_config.get(\"catalog\")\n",
    "schema = global_config.get(\"schema_insurance_agents\")\n",
    "\n",
    "@function_tool\n",
    "def search_claims_details_by_policy_no(wrapper: RunContextWrapper[UserInfo]) -> FunctionExecutionResult:\n",
    "    print(\"[DEBUG]: the 'search_claims_details_by_policy_no' tool was called\")\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=f\"{catalog}.{schema}.search_claims_details_by_policy_no\",\n",
    "        parameters={\"input_policy_no\": wrapper.context.policy_no},\n",
    "    )\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def policy_docs_vector_search(query: str) -> FunctionExecutionResult:\n",
    "    print(\"[DEBUG]: the 'policy_docs_vector_search' tool was called\")\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=f\"{catalog}.{schema}.policy_docs_vector_search\",\n",
    "        parameters={\"query\": query},\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c74df770-6ee0-47ea-8d18-418e813359de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Create Agents"
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a36beeb-989c-4e8f-8c2f-60fcd4efc64b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T06:31:31.786878Z",
     "start_time": "2025-05-12T06:31:31.749284Z"
    }
   },
   "source": [
    "# If you want custom model hosted outside of OpenAI\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from agents import OpenAIChatCompletionsModel, set_tracing_disabled\n",
    "\n",
    "# You can replace 'gtp-4o' with the MODEL variable in the Agent definition\n",
    "MODEL = \"databricks-claude-3-7-sonnet\"\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=f\"https://{DATABRICKS_HOST}/serving-endpoints\",\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The instruction field is the prompt for the agent. We’ve added a `RECOMMENDED_PROMPT_PREFIX` object from OpenAI. This is a optional “hand-off” prompt, and we found it to work fairly well. The tools field is where you designate a list of tools that the agent can use to complete the task specified in the instruction. The model field is where you specify the LLM that powers the agent. It works natively with all OpenAI models, provided that you have access to them"
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af856cc-cce3-44dc-ad7e-808809971a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T06:33:30.013195Z",
     "start_time": "2025-05-12T06:33:29.852262Z"
    }
   },
   "source": [
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "\n",
    "RECOMMENDED_PROMPT_PREFIX"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# System context\\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da7c66b-a35e-4cbc-b02a-024ce2bd6f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T06:33:32.693336Z",
     "start_time": "2025-05-12T06:33:32.666964Z"
    }
   },
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "\n",
    "# You can turn off trace by setting this to True\n",
    "set_tracing_disabled(disabled=False)\n",
    "\n",
    "claims_detail_retrieval_agent = Agent[UserInfo](\n",
    "    name=\"Claims Details Retrieval Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a claims details retrieval agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer. \\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to retrieve data about a claim. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[\n",
    "        search_claims_details_by_policy_no,\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    # model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),\n",
    ")\n",
    "\n",
    "policy_qa_agent = Agent[UserInfo](\n",
    "    name=\"Policy Q&A Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are an insurance policy Q&A agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer.\\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to answer the question about their policy. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[policy_docs_vector_search],\n",
    "    model=\"gpt-4o\",\n",
    "    # model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),\n",
    ")\n",
    "\n",
    "triage_agent = Agent[UserInfo](\n",
    "    name=\"Triage agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a helpful triaging agent. \"\n",
    "        \"You can use your tools to delegate questions to other appropriate agents. \"\n",
    "        \"If the customer does not have anymore questions, wish them a goodbye and a good rest of their day. \"\n",
    "    ),\n",
    "    # handoffs=[customer_verifier_agent, claims_detail_retrieval_agent, policy_qa_agent],\n",
    "    handoffs=[claims_detail_retrieval_agent, policy_qa_agent],\n",
    "    model=\"gpt-4o\",\n",
    "    # model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "392e61ea-9954-4cda-b051-7e968bf1129c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment a Chat loop with Mlflow\n",
    "\n",
    "Sample Conversation:\n",
    "* \"hi, id like to check on my existing claims\"\n",
    "* \"here's the policy number: 102070455\"\n",
    "* \"sure. id like to ask a different question. does my policy cover towing and labor costs?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4572519-b109-4ee6-9963-7dc946197b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T02:34:33.484988Z",
     "start_time": "2025-05-12T02:34:33.293745Z"
    }
   },
   "source": [
    "# Input some user data as context\n",
    "user_info = UserInfo(cust_id=\"7852\", policy_no=\"102070455\", coversation_id=\"123\", user_id=\"123\")\n",
    "user_input = \"[USER]: I'like to check on my existing claims\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9345cb-7f77-4b44-9f32-55701dffa315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T03:11:59.863545Z",
     "start_time": "2025-05-12T03:10:39.134011Z"
    }
   },
   "source": [
    "# Start a chat span\n",
    "with mlflow.start_span(name=\"insurance_agent\", span_type=\"AGENT\") as span:\n",
    "    print(\"[AGENT] Hello! How may I assist you?\")\n",
    "    result = None\n",
    "    while True:\n",
    "        user_input = input(\"[USER]: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"[AGENT]: Bye!\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            continue\n",
    "        try:\n",
    "            if result:\n",
    "                user_input = result.to_input_list() + [{\"role\": \"user\", \"content\": user_input}]\n",
    "            result = await Runner.run(\n",
    "                starting_agent=triage_agent, input=user_input, context=user_info\n",
    "            )\n",
    "            print(\"\\n[AGENT]:\", result.final_output)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError occurred: {str(e)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGENT] Hello! How may I assist you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AGENT]: To check an existing claim, please provide your policy number.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AGENT]: Thank you! Let me retrieve the details for policy number 102070455. One moment, please.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:databricks.sdk:loading DEFAULT profile from ~/.databrickscfg: host, serverless_compute_id, connection_timeout_seconds, token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG]: the 'search_claims_details_by_policy_no' tool was called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n",
      "INFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AGENT]: Here are the claim details for policy number 102070455:\n",
      "\n",
      "1. **Claim Number:** f82bf191-fb74-483c-81c6-8ae9c5cdd778\n",
      "   - **Claim Date:** 2017-01-09\n",
      "   - **Incident Date:** 2017-01-06\n",
      "   - **Incident Type:** Single Vehicle Collision\n",
      "   - **Collision Type:** Side Collision\n",
      "   - **Incident Severity:** Minor Damage\n",
      "   - **Driver Age:** 28\n",
      "   - **Driver Insured Relationship:** Husband\n",
      "   - **Claim Amounts:** \n",
      "     - Injury: $15,500\n",
      "     - Property: $7,750\n",
      "     - Vehicle: $62,000\n",
      "     - Total: $85,250\n",
      "\n",
      "2. **Claim Number:** 1c1187c0-d6fb-44ca-9b89-19d6b37ed015\n",
      "   - **Claim Date:** 2017-01-09\n",
      "   - **Incident Date:** 2017-01-07\n",
      "   - **Incident Type:** Single Vehicle Collision\n",
      "   - **Collision Type:** Front Collision\n",
      "   - **Incident Severity:** Minor Damage\n",
      "   - **Driver Age:** 43\n",
      "   - **Driver Insured Relationship:** Other relative\n",
      "   - **Claim Amounts:** \n",
      "     - Injury: $7,530\n",
      "     - Property: $15,060\n",
      "     - Vehicle: $60,240\n",
      "     - Total: $82,830\n",
      "\n",
      "If you need further details, feel free to ask!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AGENT]: Your policy number is 102070455. If you have any more questions about your policy, feel free to ask!\n",
      "[AGENT]: Bye!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Trace(request_id=fd0cee72bd1e46d9a1660676eb7f060c)"
      ],
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=fd0cee72bd1e46d9a1660676eb7f060c&amp;experiment_id=587749466365911461&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6955e74-dc41-4c83-8db7-db71f42359f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "# Write the agent to a file"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:33:40.865014Z",
     "start_time": "2025-05-12T06:33:40.837875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile insurance_chat_agent.py\n",
    "from typing import Any, List, Optional, Dict, Generator\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "import mlflow\n",
    "from uuid import uuid4\n",
    "import asyncio\n",
    "from pydantic import BaseModel\n",
    "from unitycatalog.ai.core.databricks import (\n",
    "    DatabricksFunctionClient,\n",
    "    FunctionExecutionResult,\n",
    ")\n",
    "from agents import function_tool, RunContextWrapper\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"insurance_chat_agent\")\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"{{secrets/databricks_token_qyu/OpenAi}}\"\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    cust_id: str | None = None\n",
    "    policy_no: str | None = None\n",
    "    conversation_id: str | None = None\n",
    "    user_id: str | None = None\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_claims_details_by_policy_no(wrapper: RunContextWrapper[UserInfo], policy_no: str) -> FunctionExecutionResult:\n",
    "    logger.info(\"The 'search_claims_details_by_policy_no' tool was called\")\n",
    "    wrapper.context.policy_no = policy_no\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=\"ai.insurance_agent.search_claims_details_by_policy_no\",\n",
    "        parameters={\"input_policy_no\": wrapper.context.policy_no},\n",
    "    )\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def policy_docs_vector_search(query: str) -> FunctionExecutionResult:\n",
    "    logger.info(\"The 'policy_docs_vector_search' tool was called\")\n",
    "    client = DatabricksFunctionClient()\n",
    "    return client.execute_function(\n",
    "        function_name=\"ai.insurance_agent.policy_docs_vector_search\",\n",
    "        parameters={\"query\": query},\n",
    "    )\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "\n",
    "claims_detail_retrieval_agent = Agent[UserInfo](\n",
    "    name=\"Claims Details Retrieval Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a claims details retrieval agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer. \\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to retrieve data about a claim. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[\n",
    "        search_claims_details_by_policy_no,\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "policy_qa_agent = Agent[UserInfo](\n",
    "    name=\"Policy Q&A Agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are an insurance policy Q&A agent. \"\n",
    "        \"If you are speaking to a customer, you probably were transferred to you from the triage agent. \"\n",
    "        \"Use the following routine to support the customer.\\n\"\n",
    "        \"# Routine: \\n\"\n",
    "        \"1. Identify the last question asked by the customer. \\n\"\n",
    "        \"2. Use the search tools to answer the question about their policy. Do not rely on your own knowledge. \\n\"\n",
    "        \"3. If you cannot answer the question, transfer back to the triage agent. \\n\"\n",
    "    ),\n",
    "    tools=[policy_docs_vector_search],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "triage_agent = Agent[UserInfo](\n",
    "    name=\"Triage agent\",\n",
    "    instructions=(\n",
    "        f\"{RECOMMENDED_PROMPT_PREFIX}\"\n",
    "        \"You are a helpful triaging agent. \"\n",
    "        \"You can use your tools to delegate questions to other appropriate agents. \"\n",
    "        \"If the customer does not have anymore questions, wish them a goodbye and a good rest of their day. \"\n",
    "    ),\n",
    "    handoffs=[claims_detail_retrieval_agent, policy_qa_agent],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "class InsuranceChatAgent(ChatAgent):\n",
    "    def __init__(self, starting_agent: Agent):\n",
    "        self.starting_agent = starting_agent\n",
    "        self.conversation_state = {}\n",
    "\n",
    "    def _get_or_create_conversation_state(self, conversation_id: str):\n",
    "        \"\"\"Get or create the state for a conversation\"\"\"\n",
    "        if conversation_id not in self.conversation_state:\n",
    "            self.conversation_state[conversation_id] = {\n",
    "                \"current_agent\": self.starting_agent,\n",
    "                \"conversation_history\": None\n",
    "            }\n",
    "        return self.conversation_state[conversation_id]\n",
    "\n",
    "    def _get_latest_user_message(selfself, messages: List[ChatAgentMessage]) -> str:\n",
    "        \"\"\"Extract the most recent user messages as input text\"\"\"\n",
    "        for message in reversed(messages):\n",
    "            if message.role == \"user\":\n",
    "                return message.content\n",
    "            return \"\"\n",
    "\n",
    "    def _create_user_context(\n",
    "            self,\n",
    "            context: Optional[ChatContext] = None,\n",
    "            custom_inputs: Optional[Dict[str, Any]] = None\n",
    "        ) -> UserInfo:\n",
    "        \"\"\"Convert MLflow inputs to UserInfo object\"\"\"\n",
    "        user_info = UserInfo()\n",
    "\n",
    "        if context:\n",
    "            conversation_id = getattr(context, \"conversation_id\", None)\n",
    "            if conversation_id:\n",
    "                user_info.conversation_id = conversation_id\n",
    "\n",
    "            user_id = getattr(context, \"user_id\", None)\n",
    "            if user_id:\n",
    "                user_info.user_id = user_id\n",
    "\n",
    "        return user_info\n",
    "\n",
    "    @mlflow.trace(name=\"insurance_chat_agent\", span_type=SpanType.AGENT)\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[Dict[str, Any]] = None\n",
    "    ) -> ChatAgentResponse:\n",
    "        user_info = self._create_user_context(context, custom_inputs)\n",
    "        conversation_id = user_info.conversation_id\n",
    "\n",
    "        # Get the state for this conversation\n",
    "        state = self._get_or_create_conversation_state(conversation_id)\n",
    "        current_agent = state[\"current_agent\"]\n",
    "        conversation_history = state[\"conversation_history\"]\n",
    "\n",
    "\n",
    "        # Get the latest user message\n",
    "        latest_message = self._get_latest_user_message(messages)\n",
    "\n",
    "        # Prepare the input for the agent\n",
    "        if conversation_history is None:\n",
    "            # First turn, just use the latest message\n",
    "            agent_input = latest_message\n",
    "        else:\n",
    "            # Add the new user message to the conversation history\n",
    "            conversation_history.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": latest_message\n",
    "            })\n",
    "            agent_input = conversation_history\n",
    "\n",
    "        # Run the agent use asyncio\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        try:\n",
    "            result = loop.run_until_complete(\n",
    "                Runner.run(\n",
    "                    starting_agent=self.starting_agent,\n",
    "                    input=agent_input,\n",
    "                    context=user_info,\n",
    "                )\n",
    "            )\n",
    "            # Update the state for the next turn\n",
    "            # Store the updated conversation history from the result\n",
    "            state[\"conversation_history\"] = result.to_input_list()\n",
    "\n",
    "            # Update the current agent based on which agent was last used\n",
    "            if hasattr(result, \"last_agent\") and result.last_agent:\n",
    "                state[\"current_agent\"] = result.last_agent\n",
    "\n",
    "        finally:\n",
    "            loop.close()\n",
    "\n",
    "        # Convert the result to ChatAgentResponse format:\n",
    "        return ChatAgentResponse(\n",
    "            messages=[\n",
    "                ChatAgentMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=result.final_output,\n",
    "                    id=str(uuid4())\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @mlflow.trace(name=\"insurance_change_agent_stream\", span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[Dict[str, Any]] = None\n",
    "    ) -> Generator[ChatAgentResponse, None, None]:\n",
    "        response = self.predict(messages, context, custom_inputs)\n",
    "\n",
    "        # Yield it as a single chunk\n",
    "        for message in response.messages:\n",
    "            yield ChatAgentChunk(delta=message)\n",
    "\n",
    "AGENT = InsuranceChatAgent(starting_agent=triage_agent)\n",
    "mlflow.models.set_model(AGENT)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing insurance_chat_agent.py\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Agent, Run Validation, log and Register the Agent Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:37:42.561469Z",
     "start_time": "2025-05-12T06:37:30.167421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from insurance_chat_agent import AGENT\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "AGENT.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"hi, id like to check on my existing claims and my policy number: 102070455\",\n",
    "            }\n",
    "        ],\n",
    "        \"context\": {\"conversation_id\": \"123\", \"user_id\": \"123\"},\n",
    "    }\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:insurance_chat_agent:The 'search_claims_details_by_policy_no' tool was called\n",
      "INFO:databricks.sdk:loading DEFAULT profile from ~/.databrickscfg: host, serverless_compute_id, connection_timeout_seconds, token\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatAgentResponse(messages=[ChatAgentMessage(role='assistant', content='I’m unable to retrieve your claim details at the moment due to a system error. Please try again later or contact customer support for immediate assistance.', name=None, id='c6e1724e-b9de-4628-a0d1-8343a0c68b00', tool_calls=None, tool_call_id=None, attachments=None)], finish_reason=None, custom_outputs=None, usage=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Trace(request_id=649055a0a8d14a03be2d5107d05b7a85)"
      ],
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=649055a0a8d14a03be2d5107d05b7a85&amp;experiment_id=587749466365911461&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "AGENT.predict({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"does my policy cover towing and labor costs?\"}],\n",
    "        \"context\": {\"conversation_id\": \"123\", \"user_id\": \"123\"}\n",
    "})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Log the agent"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import mlflow\n",
    "import os\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksFunction,\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksVectorSearchIndex)\n",
    "from unitycatalog.ai.openai.toolkit import UCFunctionToolkit\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\n",
    "    scope=\"my_secret_scope\", key=\"OpenAi\"\n",
    ")\n",
    "\n",
    "resources = [\n",
    "    DatabricksVectorSearchIndex(\n",
    "        index_name=\"ai.agents.policy_docs_chunked_files_vs_index\"\n",
    "    ),\n",
    "    DatabricksServingEndpoint(endpoint_name=\"databricks-bge-large-en\"),\n",
    "    DatabricksFunction(\n",
    "        function_name=\"ai.insurance_agent.search_claims_details_by_policy_no\"\n",
    "    ),\n",
    "    DatabricksFunction(\n",
    "        function_name=\"ai.insurance_agent.policy_docs_vector_search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{my_databricks_account}/ML_experiments/insurance_chat_agent\")\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Log the model using the \"models from code\" approach\n",
    "with mlflow.start_run():\n",
    "    logged_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"insurance_chat_agent\",\n",
    "        python_model=os.path.join(os.getcwd(), \"insurance_chat_agent.py\"),\n",
    "        input_example={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"hi, id like to check on my existing claims?\",\n",
    "                }\n",
    "            ],\n",
    "            \"context\": {\"conversation_id\": \"123\", \"user_id\": \"123\"},\n",
    "        },\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"openai-agents\",\n",
    "            \"unitycatalog-openai[databricks]==0.2.0\",\n",
    "            \"pydantic\",\n",
    "        ],\n",
    "        resources=resources\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1412556571878803,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) insurance-agent-with-tools",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
